{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем датасет из Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/joannpeeler/labeled-chess-positions-109m-csv-format?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.25G/4.25G [22:23<00:00, 3.40MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/user0/.cache/kagglehub/datasets/joannpeeler/labeled-chess-positions-109m-csv-format/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Загрузка датасета\n",
    "path = kagglehub.dataset_download(\"joannpeeler/labeled-chess-positions-109m-csv-format\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import (\n",
    "    regexp_replace,\n",
    "    regexp_extract_all,\n",
    "    col,\n",
    "    lit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_configuration() -> SparkConf:\n",
    "    \"\"\"\n",
    "    Создает и конфигурирует экземпляр SparkConf для приложения Spark.\n",
    "\n",
    "    Returns:\n",
    "        SparkConf: Настроенный экземпляр SparkConf.\n",
    "    \"\"\"\n",
    "    # Получаем имя пользователя\n",
    "    user_name = os.getenv(\"USER\")\n",
    "    \n",
    "    conf = SparkConf()\n",
    "    conf.setAppName(\"lab 1 Test\")\n",
    "    conf.setMaster(\"yarn\")\n",
    "    conf.set(\"spark.submit.deployMode\", \"client\")\n",
    "    conf.set(\"spark.executor.memory\", \"12g\")\n",
    "    conf.set(\"spark.executor.cores\", \"8\")\n",
    "    conf.set(\"spark.executor.instances\", \"2\")\n",
    "    conf.set(\"spark.driver.memory\", \"4g\")\n",
    "    conf.set(\"spark.driver.cores\", \"2\")\n",
    "    conf.set(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.0\")\n",
    "    conf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    conf.set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\")\n",
    "    conf.set(\"spark.sql.catalog.spark_catalog.warehouse\", f\"hdfs:///user/{user_name}/warehouse\")\n",
    "    conf.set(\"spark.sql.catalog.spark_catalog.io-impl\", \"org.apache.iceberg.hadoop.HadoopFileIO\")\n",
    "\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = create_spark_configuration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.5.2-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/user0/.ivy2/cache\n",
      "The jars for the packages stored in: /home/user0/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7b35bfbd-f28f-45df-b464-4b9ae98149e6;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.0 in central\n",
      ":: resolution report :: resolve 601ms :: artifacts dl 25ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.6.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7b35bfbd-f28f-45df-b464-4b9ae98149e6\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/24ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/13 23:28:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/01/13 23:28:51 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/01/13 23:28:51 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/01/13 23:28:51 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/01/13 23:28:51 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/01/13 23:28:51 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/01/13 23:28:51 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "25/01/13 23:28:51 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "25/01/13 23:28:51 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "25/01/13 23:28:55 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "25/01/13 23:29:13 WARN Client: Same path resource file:///home/user0/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.6.0.jar added multiple times to distributed cache.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://node32.cluster:4049\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lab 1 Test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fab4e75cd10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = \"hdfs:///user/user0/master_training_data_ver7.0d.csv\"\n",
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .load(path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+-------+--------------------+----------+----+------+\n",
      "|            Hash|Ply|GamePly|                 FEN|HasCastled|Eval|Result|\n",
      "+----------------+---+-------+--------------------+----------+----+------+\n",
      "|77C51D9E08C4CFE5| 44|    113|3r1r1k/1pq2pp1/p4...|         3| -21|   0.5|\n",
      "|40E22C66F91971B5| 44|     97|2q3rk/1rnbppbn/p5...|         3| 223|   1.0|\n",
      "|05F5B9C1E13B6DB2| 55|    204|r2br2k/3b1pp1/2p4...|         3|-111|   0.0|\n",
      "|B7F72DC9E8B257F2| 97|    131|8/3b1pk1/3Q2p1/1p...|         3|-252|   1.0|\n",
      "|D30ED7CEEA0347BF| 45|    105|3r2k1/1p3ppp/rnp1...|         3| 135|   0.5|\n",
      "|5FBA2109603D3D66|139|    162|8/4k3/r6p/4P1pP/4...|         3|-835|   0.0|\n",
      "|B616FCC024D2C623|165|    178|8/5K2/3k4/5P2/4R3...|         3| 412|   1.0|\n",
      "|6C02A2DF1DFF27D4| 71|    137|8/2p3b1/1p2k2p/4p...|         0| 342|   1.0|\n",
      "|FF9F28B98001BC81| 88|    110|2r2rk1/5p2/b3p2p/...|         3|   4|   1.0|\n",
      "|C804494384061CA0| 26|    207|r1bq1rk1/pp2b1pp/...|         3| 119|   1.0|\n",
      "|BC575E8C388FDC8F| 13|    136|rnbq1rk1/p1p1bppp...|         2| -12|   0.0|\n",
      "|09A8F052553A1F05| 85|    168|8/5pk1/3q2p1/7p/5...|         3|-325|   0.0|\n",
      "|FA217B6A6E22831E| 62|     94|2r5/1p4k1/p1r3pp/...|         3|-819|   0.0|\n",
      "|EE5CA779D9009B8E| 82|    146|8/7p/6p1/8/p1N4n/...|         3|-375|   0.0|\n",
      "|A17F024BEDDBD093|138|    151|8/bp6/2p1k3/P3n1p...|         1| -85|   0.5|\n",
      "|4A73470F5753CA95| 41|    112|r1b1rnk1/2qnbpp1/...|         3| 249|   1.0|\n",
      "|285EB8915A6AEAC3| 87|    371|8/5pk1/1q4p1/3p3p...|         3| -22|   0.5|\n",
      "|913090CEFB57B26F| 79|     96|4r1k1/2r3pp/1p2pp...|         3| -69|   0.5|\n",
      "|11171943F8C20771| 79|    113|8/q4pk1/1p3n2/4NP...|         3| 330|   1.0|\n",
      "|9593FA85C2B786D9| 92|    133|8/r5k1/3NQnp1/8/8...|         3| 465|   1.0|\n",
      "+----------------+---+-------+--------------------+----------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Hash: string (nullable = true)\n",
      " |-- Ply: string (nullable = true)\n",
      " |-- GamePly: string (nullable = true)\n",
      " |-- FEN: string (nullable = true)\n",
      " |-- HasCastled: string (nullable = true)\n",
      " |-- Eval: string (nullable = true)\n",
      " |-- Result: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Hash: Строка. Уникальные строки (например, хеши). \n",
    "    Уникальный идентификатор позиции на шахматной доске.\n",
    "\n",
    "-- Ply: Целое число. От 0 и выше (обычно от 0 до 100, в зависимости от хода). \n",
    "    Количество полуходов (ходов для белых и черных) в текущей игре. Это число увеличивается на 1 с каждым ходом.\n",
    "\n",
    "-- GamePly: Целое число. От 1 и выше (обычно от 1 до 200 и более). \n",
    "    Номер полного хода в игре (для белых и черных). Например, если в игре было сделано 10 полных ходов, то GamePly будет 10.\n",
    "\n",
    "-- FEN: Строка. Строки в формате FEN. \n",
    "    Представление позиции на доске в формате Forsyth-Edwards Notation (FEN), которое включает положение фигур, кто ходит, и возможность рокировки.\n",
    "\n",
    "-- HasCastled: Целое число. 0 или 3. Показывает, было ли сделано рокирование в игре: \n",
    "    0 — не было, \n",
    "    3 — было (для белых или черных, в зависимости от контекста).\n",
    "\n",
    "-- Eval: Число с плавающей точкой. От -∞ до +∞.\n",
    "    Оценка текущей позиции в терминах силы (например, положительное значение для белых, отрицательное — для черных).\n",
    "\n",
    "-- Result: Число с плавающей точкой. 0.0, 0.5, 1.0.\n",
    "    0.0 — поражение для игрока, чей ход зафиксирован,\n",
    "    0.5 — ничья,\n",
    "    1.0 — победа для игрока, чей ход зафиксирован."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+----+------+\n",
      "|Ply|GamePly|HasCastled|Eval|Result|\n",
      "+---+-------+----------+----+------+\n",
      "| 44|    113|         3| -21|   0.5|\n",
      "| 44|     97|         3| 223|   1.0|\n",
      "| 55|    204|         3|-111|   0.0|\n",
      "| 97|    131|         3|-252|   1.0|\n",
      "| 45|    105|         3| 135|   0.5|\n",
      "|139|    162|         3|-835|   0.0|\n",
      "|165|    178|         3| 412|   1.0|\n",
      "| 71|    137|         0| 342|   1.0|\n",
      "| 88|    110|         3|   4|   1.0|\n",
      "| 26|    207|         3| 119|   1.0|\n",
      "| 13|    136|         2| -12|   0.0|\n",
      "| 85|    168|         3|-325|   0.0|\n",
      "| 62|     94|         3|-819|   0.0|\n",
      "| 82|    146|         3|-375|   0.0|\n",
      "|138|    151|         1| -85|   0.5|\n",
      "| 41|    112|         3| 249|   1.0|\n",
      "| 87|    371|         3| -22|   0.5|\n",
      "| 79|     96|         3| -69|   0.5|\n",
      "| 79|    113|         3| 330|   1.0|\n",
      "| 92|    133|         3| 465|   1.0|\n",
      "+---+-------+----------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Уберем лишние колонки\n",
    "df = df.select(\n",
    "    \"Ply\", \"GamePly\", \"HasCastled\", \"Eval\", \"Result\"\n",
    ")\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Уберем колонку Hash (Уникальный идентификатор позиции) и FEN (Представление позиции на шахматной доске в формате FEN), которые "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поменяем изначальный строковый тип данных на подходящий\n",
    "\n",
    "from pyspark.sql.functions import col, regexp_replace, regexp_extract_all, lit\n",
    "\n",
    "def transform_dataframe(data: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Преобразует столбцы DataFrame в указанные типы данных и\n",
    "    выполняет необходимые преобразования.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): Исходный DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Преобразованный DataFrame.\n",
    "    \"\"\"\n",
    "    # Преобразуем столбцы в соответствующие типы данных\n",
    "    data = data.withColumn(\"Ply\", col(\"Ply\").cast(\"Integer\"))\n",
    "    data = data.withColumn(\"GamePly\", col(\"GamePly\").cast(\"Integer\"))\n",
    "    data = data.withColumn(\"HasCastled\", col(\"HasCastled\").cast(\"Integer\"))\n",
    "    data = data.withColumn(\"Eval\", col(\"Eval\").cast(\"Float\"))\n",
    "    data = data.withColumn(\"Result\", col(\"Result\").cast(\"Float\"))\n",
    "\n",
    "    # Возвращаем преобразованный DataFrame\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ply: integer (nullable = true)\n",
      " |-- GamePly: integer (nullable = true)\n",
      " |-- HasCastled: integer (nullable = true)\n",
      " |-- Eval: float (nullable = true)\n",
      " |-- Result: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Проверим измененный тип данных\n",
    "df = transform_dataframe(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала создадим базу данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = \"samorokov_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим инструкцию SQL для добавления базы данных в каталог Apache Spark\n",
    "create_database_sql = f\"\"\"\n",
    "CREATE DATABASE IF NOT EXISTS spark_catalog.{database_name}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(create_database_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установим созданную базу данных как текущую.\n",
    "spark.catalog.setCurrentDatabase(database_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Сохранение DataFrame в виде таблицы\n",
    "# записываем преобразованный датафрейм в таблицу sobd_lab1_table\n",
    "df.writeTo(\"sobd_table_lab1\").using(\"iceberg\").create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "БД успешно создана, глянем, какие таблицы туда входят"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sobd_table_lab1\n"
     ]
    }
   ],
   "source": [
    "for table in spark.catalog.listTables():\n",
    "    print(table.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПРИМЕР (удаления таблицы):\n",
    "##### spark.sql(\"DROP TABLE spark_catalog.ivanov_database.sobd_lab1_table\")\n",
    "##### spark.sql(\"DROP DATABASE spark_catalog.ivanov_database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# После удачной записи остановим сессию\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
