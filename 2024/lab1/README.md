## Лабораторная работа № 1
### Выполнение разведочного анализа больших данных с использованием фреймворка Apache Spark
#### Цель работы:
* Выполнить разведочный анализ датасета большого объёма с помощью фреймворка `Apache Spark`.

#### Задачи работы:
1. Познакомиться с понятием «большие данные» и способами их обработки.
2. Познакомиться с инструментом `Apache Spark` и возможностями, которые он предоставляет для обработки больших данных.
3. Получить представление об инструментах экосистемы `Hadoop`: HDFS и YARN.
4. Поработать с табличным форматом для больших данных `Apache Iceberg`.
4. Получить навыки выполнения разведочного анализа данных использованием `pyspark`.

#### Порядок выполнения работы:
1. Установите среду разработки [Microsoft Visual Studio Code](https://code.visualstudio.com/).
2. Запустите `VS Code`, на левой панели откройте вкладку `Extensions` и установите расширения `Python`, `Jupyter`, `Remote - SSH`.
3. На левой панели откройте вкладку `Remote Explorer`, наведите мышкой на строку `SSH` и щелкните знак `+`.
4. В открывшейся строке наберите следующую команду, заменив `user`, `ip` и `port` на параметры учетной записи. Учетные записи на кластере доступны в ЭИОС. Если система сообщает, что порт `12345` занят, вместо него используйте любой свободный.

```
ssh -D 12345 -J user@ip:port user@hadoop-node
```

После введения строки укажите файл `~/.ssh/config` в качестве хранилища записи.

После этого в файле `~/.ssh/config` появится запись вида:

```
Host hadoop-node
  HostName hadoop-node
  DynamicForward 12345
  ProxyJump user@ip:port
  User user
```

5. Щелкните по кнопке с изображением стрелки на появившемся хосте. Если VS Code запросит платформу удалённого узла, укажите `Linux`.
   
6. При запросе пароля от учетной введите его (возможно, дважды). Дождитесь соединения с кластером и настройки удалённого сервера.
   
7. Установите на удаленный сервер расширения `Python` и `Jupyter` тем же способом, что и ранее (убедитесь, что рядом с именем расширения указано `Install in SSH: hadoop-node`)
   
8. Нажмите на вкладку `Terminal` на панели инструментов `VS Code` и откройте новый терминал.
   
9. Проверьте доступность файловой системы `HDFS`, набрав команду:

```
hdfs dfs -ls /
```

Побродите по файловой системе `HDFS`, обратите внимание на права доступа к файлам и каталогам.

Проверьте объём оставшегося дискового пространства `HDFS`, набрав команду:

```
hdfs dfs -df -h
```

10. Проверьте доступность системы `YARN` командой:

```
yarn node -list
```

11. Проверьте загрузку очереди `YARN`, набрав команду:

```
yarn application -list
```

Ненужное приложение можно остановить командой:

```
yarn application -kill <app-id>
```

12. Настройте браузер для использования `SOCKS5 Proxy`. Проще всего это сделать в браузере [Mozilla Firefox](https://www.mozilla.org/ru/firefox/new/), у которого есть настройки прокси, отличные от системных. В настройках укажите:

* Тип прокси для доступа в Интернет: `Ручная настройка прокси`.

* Узел SOCKS: `localhost`.
  
* Порт: `12345` -- это порт, который Вы указали в опции -D при настройке соединения с кластером.
  
* Тип прокси: `SOCKS 5`.

13. Теперь в браузере должны быть доступны веб-страницы сервисов `Hadoop` и `Spark`:

| Служба               | Адрес                       |
| -------------------- | --------------------------- |
| Name Node            | <http://192.168.1.32:9870>  |
| Resource Manager     | <http://192.168.1.32:8088>  |
| Spark History Server | <http://192.168.1.32:18080> |

14. Исследуйте содержимое представленных страниц, оцените информацию, которую удалось получить.

15. Создайте **свою директорию** в локальной директории пользователя на кластере:

```
mkdir ivanov_directory
```

и перейдите в неё:

```
cd ivanov_directory
```

16. Склонируйте репозиторий с лабораторными работами:

```
git clone https://github.com/kpdvstu/SOBD.git
```

17. На левой панели `VS Code` нажмите на кнопку `Open Folder` и перейдите в директорию с первой лабораторной работой.
    
18. Откройте файлы `Jupyter Notebook` (*.ipynb), выберите виртуальное окружение `pyspark-env` и поочерёдно запустите инструкции в файлах на исполнение. Некоторые инструкции потребуют модификации информации о запустившем их пользователе, не пропустите их.
    
* **Совет**. В процессе работы с ноутбуком удобно запустить в терминале команду, позволяющую отслеживать в реальном времени все работающие задачи в `YARN`:

```
watch yarn application -list
```

19. **Выполните аналогичным образом разведочный анализ датасета по Вашему выбору** с определением:
* типов признаков в датасете;
* пропущенных значений и их устранением;
* выбросов и их устранением;
* расчетом статистических показателей признаков (средних, квартилей и т.д.);
* визуализацией распределения наиболее важных признаков;
* корреляций между признаками.

**Обратите внимание**:
* Объём датасета -- не менее **нескольких гигабайт**.
* Датасеты можно выбрать на [Kaggle](https://www.kaggle.com/datasets) (для скачивания датасета требуется регистрация).
* Перемещать файлы между локальной файловой системой Вашего компьютера и локальной файловой системой удаленного узла кластера, к которому Вы подключились, можно с помощью программы [WinSCP](https://winscp.net/eng/download.php). Также можно просто перетащить файлы мышкой в нужную папку в `VS Code`.
* Загрузить файлы с локальной файловой системой удаленного узла кластера в распределенную файловую систему `HDFS` можно с помощью следующей команды:

```
hdfs dfs -copyFromLocal /home/ivanov/dataset/data/ data
```

* А загрузить файлы с `HDFS` в локальную директорию удаленного узла -- командой:

```
hdfs dfs -copyToLocal  data /home/ivanov/dataset/data/
```

20. Сделайте выводы по работе.
21. Напишите **главу курсовой работы** в пределах **15-20 страниц**, в которой опишите постановку задачи, описание Вашего датасета со ссылкой на него, проведенный разведовательный анализ и выводы.
22. Сохраните ноутбук с проведенным анализом и написанную главу курсовой в `GitHub / GitLab`.

**К отчету** следует представить репозиторий на `GitHub / GitLab` с выполненным разведочным анализом и главой курсовой работы, а также быть готовым продемонстрировать работоспособность кода и пояснить спорные моменты.

#### Список теоретических вопросов к отчету:

### Список вопросов будет пополняться и изменяться после лекций!

1. Четвёртая промышленная революция. Индустрия 4.0 и основные её составляющие. Роль данных в индустрии 4.0.
2. Понятие больших данных. Модель `mV` для больших данных. Источники, основные методы анализа и применение больших данных.
3. Транзакционные и аналитические базы данных, характер эффективной нагрузки для них и сценарии использования. Примеры.
4. Понятие NoSQL-баз данных, их особенности, применимость для хранения и обработки больших данных. Представители NoSQL-баз данных.
5. Понятие озера данных. Управление данными в озере, "болото данных". Сравнительный анализ озер данных и DWH.
6. Поколения озёр данных, их особенности и технологии, лежащие в основе их функционирования. Архитектура LakeHouse.
7. Фреймворк `Apache Hadoop`, его основные модули. Экосистема `Hadoop` и основные её представители.
8. Система `HDFS`: определение и свойства. Организация `HDFS`. Консольный клиент `HDFS` и его основные команды.
9. Архитектура `HDFS`, основные службы, входящие в её состав, и их роли. Режим высокой доступности.
10. Организация отказоустойчивости. Понятие `heartbeat`. Репликация, коэффициент репликации.
11. Система `YARN`: понятие и назначение. Преимущества `YARN` над планировщиком Hadoop первой версии.
12. Архитектура `YARN`, основные службы, входящие в её состав, и их роли.
13. Жизненный цикл работы приложения в `YARN`. Организация отказоустойчивости.
14. Режимы работы планировщика `YARN`, их особенности.
15. Модель `MapReduce`, принципы работы этапов `Map` и `Reduce`. Система `Apache Hive`.
16. Фреймворк обработки больших данных `Apache Spark`, его назначение, функции и отличия от `Hadoop MapReduce`.
17. Понятие устойчивого распределенного набора данных (RDD). Понятие раздела RDD (partition). Способы создания RDD. Трансформации (transformations) и действия (actions). Кэширование (cache) данных в Spark.
18. RDD и PairRDD: понятие, назначение. Основные трансформации (transformations) и действия (actions) над ними.
19. Реализация концепции MapReduce в фреймворке Spark. Функции map, flatMap, mapValues, mapPartitions, reduce, reduceByKey.
20. Модели запуска Spark-приложений (YARN, Standalone, Kubernetes). Понятие драйвера (driver) и исполнителей (executors). Понятия задания (job), этапа (stage) и задачи (task). Модель ленивых вычислений (lazy) и ее применение в Spark. Понятие ориентированного ациклического графа (Directed Acyclic Graph, DAG).
21. Операция перемешивания данных (shuffle): причины возникновения, влияние на производительность. Класс Partitioner. Пути повышения производительности.
22. Понятие датафрейма в Spark. Основные операции над датафреймами. Скалярные функции, агрегирующие функции, оконные функции. Оптимизация запросов. Catalyst.
23. Клиентский (client) и кластерный (cluster) режимы работы Apache Spark.

#### Литература для подготовки к отчету:
1. Изучаем Spark: молниеносный анализ данных / Х. Карау, Э. Конвински, П. Венделл, М.М. Захария // ДМК Пресс, 2015. — 304 с.: ил.
2. Data Exploration // Learning Apache Spark with Python [Электронный  ресурс] / W. Feng. - [2021]. - Режим доступа : https://runawayhorse001.github.io/LearningApacheSpark/exploration.html (дата обращ. 19.09.2022).
3. Advanced Pyspark for Exploratory Data Analysis [Электронный  ресурс]. – [2022]. – Режим доступа : https://www.kaggle.com/code/tientd95/advanced-pyspark-for-exploratory-data-analysis (дата обращ. 19.09.2022).
4. Exploratory Data Analysis (EDA) with PySpark on Databricks [Электронный  ресурс]. – [2020]. – Режим доступа : https://towardsdatascience.com/exploratory-data-analysis-eda-with-pyspark-on-databricks-e8d6529626b1 (дата обращ. 19.09.2022).
5. Exploratory data analysis with pySpark [Электронный  ресурс]. – [2020]. – Режим доступа : https://github.com/roshankoirala/pySpark_tutorial/blob/master/Exploratory_data_analysis_with_pySpark.ipynb (дата обращ. 19.09.2022).
6. Официальный сайт Apache Spark [Электронный  ресурс]. – [2022]. – Режим доступа : https://spark.apache.org/ (дата обращ. 19.09.2022).
7. Уайт, Т. Hadoop: Подробное руководство / Т. Уайт. — 3-е изд. — СПб. : Питер, 2013. — 672 с.: ил.
8. Захария, М.А. и др. “Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics” // Conference on Innovative Data Systems Research. — 2021. URL: <https://www.cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf> (дата обращ. 12.09.2024).
